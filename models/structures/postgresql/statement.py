from typing import Iterator, Dict, Optional

import psycopg2
import psycopg2.extras

from helpers.logger import logger

from models.structures.database import SQLDatabase, SQLTable, SQLColumn
from models.structures.postgresql.datatype import PostgreSQLDataType
from models.structures.statement import AbstractStatement, Transaction, LOG_QUERY


class PostgreSQLStatement(AbstractStatement):

    def __init__(self, session):
        # Parse connection details from session
        self.host = session.configuration.hostname
        self.user = session.configuration.username
        self.password = session.configuration.password
        self.database = "postgres"
        self.port = getattr(session.configuration, 'port', 5432)

        # Create connection URL for compatibility
        self.connection_url = f"postgresql://{self.user}:{self.password}@{self.host}:{self.port}"
        if self.database:
            self.connection_url += f"/{self.database}"

        super().__init__(self.connection_url)

    def _get_column_definition(self, table: SQLTable, column: SQLColumn):
        parts = [f'"{column.name}"']

        datatype_parts = str(column.datatype.name)

        if column.datatype.has_length and column.length:
            datatype_parts += f"({column.length})"
        elif column.datatype.has_precision and column.numeric_precision:
            if column.datatype.has_scale and column.numeric_scale is not None:
                datatype_parts += f"({column.numeric_precision},{column.numeric_scale})"
            else:
                datatype_parts += f"({column.numeric_precision})"

        parts.append(datatype_parts)

        if not column.is_nullable:
            parts.append("NOT NULL")

        if column.default and column.default != '':
            if column.default == "AUTO_INCREMENT":
                # PostgreSQL uses SERIAL or IDENTITY
                parts.append("GENERATED BY DEFAULT AS IDENTITY")
            else:
                parts.append(f"DEFAULT {column.default}")

        return " ".join(parts)

    def connect(self, **connect_kwargs) -> None:
        """Establish connection to PostgreSQL database using psycopg2"""
        if self._connection is None:
            try:
                self._connection = psycopg2.connect(
                    host=self.host,
                    user=self.user,
                    password=self.password,
                    database=self.database,
                    port=self.port,
                    **connect_kwargs
                )
                self._connection.autocommit = False
                self._cursor = self._connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
                self._on_connect()
            except Exception as e:
                logger.error(f"Failed to connect to PostgreSQL: {e}")
                raise

    def get_server_version(self) -> str:
        self.execute("SELECT version()")
        version = self.cursor.fetchone()
        return version['version']

    def get_server_uptime(self) -> Optional[int]:
        # PostgreSQL doesn't have a simple uptime query like MySQL
        # We can use pg_postmaster_start_time
        try:
            self.execute("SELECT extract(epoch from now() - pg_postmaster_start_time()) as uptime")
            result = self.cursor.fetchone()
            return int(result['uptime'])
        except:
            return None

    def get_databases(self) -> Iterator[SQLDatabase]:
        self.execute("""
            SELECT datname
            FROM pg_database
            WHERE datistemplate = false
        """)
        databases = self.cursor.fetchall()

        for index, row in enumerate(databases):
            database_name = row['datname']
            yield SQLDatabase(
                id=index,
                name=database_name,
                get_tables_handler=self.get_tables
            )

    def get_tables(self, database: SQLDatabase) -> Iterator[SQLTable]:
        self.execute(f"""
            SELECT table_name
            FROM information_schema.tables
            WHERE table_schema='{database.name}'
        """)
        tables_result = self.cursor.fetchall()

        for i, row in enumerate(tables_result):
            yield SQLTable(
                id=i,
                name=row['tablename'],
                database=database,
                engine='postgresql',
                get_columns_handler=self.get_columns,
            )

    def get_columns(self, database: SQLDatabase, table: SQLTable) -> Iterator[SQLColumn]:
        self.execute("""
            SELECT
                c.column_name,
                c.data_type,
                c.character_maximum_length,
                c.numeric_precision,
                c.numeric_scale,
                c.is_nullable,
                c.column_default,
                c.udt_name,
                c.ordinal_position
            FROM information_schema.columns c
            WHERE c.table_schema = %s
                AND c.table_name = %s
            ORDER BY c.ordinal_position
        """, params=(database.name, table.name))

        columns_result = self.cursor.fetchall()

        for col in columns_result:
            parsed_type = self._parse_type(col['data_type'])

            yield SQLColumn(
                id=col['ordinal_position'],
                name=col['column_name'],
                datatype=PostgreSQLDataType.get_by_name(col),
                is_nullable=col['is_nullable'] == 'YES',
                extra=None,
                key=None,
                collation_name=None,
                comment=None,
                is_unsigned=False,
                is_zerofill=False,
                virtuality=None,
                expression=None,
                server_default=col['column_default'],
                set=None,
                length=col.get('character_maximum_length', None),
                indexes=[],
                is_auto_increment=False,
                numeric_precision=parsed_type.precision if col['data_type'] in [
                    "integer", "bigint", "smallint", "numeric", "decimal"
                ] else col.get('numeric_precision', None),
                numeric_scale=parsed_type.scale,
                datetime_precision=None
            )

    def get_records(self, database : SQLDatabase, table: SQLTable, limit: int = 1000, offset: int = 0) -> Iterator[Dict]:
        query = f'SELECT * FROM "{database.name}"."{table.name}" LIMIT {limit} OFFSET {offset}'
        self.execute(query)
        results = self.cursor.fetchall()

        for row in results:
            yield dict(row)

    def build_new_table(self, database: SQLDatabase) -> SQLTable:
        return SQLTable(
            id=-1,
            name='',
            database=database,
            engine='postgresql',
            get_columns_handler=self.get_columns,
        )

    def create_table(self, database: SQLDatabase, table: SQLTable) -> bool:
        column_defs = [self._get_column_definition(table, c) for c in table.columns]

        sql = f'CREATE TABLE "{database.name}"."{table.name}" ({", ".join(column_defs)})'

        return self._execute_transaction(sql, f"create table {table.name}")

    def update_table(self, database: SQLDatabase, table: SQLTable) -> bool:
        existing_columns = self.get_columns(database, table)
        existing_columns_map = {col.name: col for col in existing_columns}

        new_columns_map = {col.name: col for col in table.columns}

        columns_to_add = []
        columns_to_modify = []
        columns_to_drop = []

        for col_name, new_col in new_columns_map.items():
            if col_name not in existing_columns_map:
                columns_to_add.append(new_col)
            else:
                existing_col = existing_columns_map[col_name]
                if existing_col != new_col:
                    columns_to_modify.append(new_col)

        columns_to_drop = [col for col_name, col in existing_columns_map.items() if col_name not in new_columns_map]

        try:
            for col in columns_to_add:
                self._add_column(table, col)

            for col in columns_to_modify:
                existing_col = existing_columns_map[col.name]
                if (existing_col.virtuality and not col.virtuality):
                    # Handle virtual column changes if needed
                    pass
                self._modify_column(table, col)

            for col in columns_to_drop:
                self._drop_column(table, col)

            return True

        except Exception as ex:
            log = f"Error altering table name={table.name}: {str(ex)}"
            logger.error(log)
            LOG_QUERY.append(log)
            return False

    def drop_table(self, database: SQLDatabase, table: SQLTable) -> bool:
        sql = f'DROP TABLE "{database.name}"."{table.name}"'
        return self._execute_transaction(sql, f"drop table {table.name}")
